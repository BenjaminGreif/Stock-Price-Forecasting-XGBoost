{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2864,
     "status": "ok",
     "timestamp": 1734722149108,
     "user": {
      "displayName": "Ben",
      "userId": "04046397739359302226"
     },
     "user_tz": -60
    },
    "id": "rO_PJdw5zY7I",
    "outputId": "a2fb0868-eb55-446a-de05-a28a3f4424b9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yfinance in /opt/anaconda3/lib/python3.12/site-packages (0.2.61)\n",
      "Requirement already satisfied: ta in /opt/anaconda3/lib/python3.12/site-packages (0.11.0)\n",
      "Requirement already satisfied: pyspark in /opt/anaconda3/lib/python3.12/site-packages (3.5.5)\n",
      "Requirement already satisfied: xgboost in /opt/anaconda3/lib/python3.12/site-packages (3.0.1)\n",
      "Requirement already satisfied: shap in /opt/anaconda3/lib/python3.12/site-packages (0.47.2)\n",
      "Requirement already satisfied: pandas>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance) (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance) (1.26.4)\n",
      "Requirement already satisfied: requests>=2.31 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance) (2.32.3)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance) (0.0.9)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance) (3.10.0)\n",
      "Requirement already satisfied: pytz>=2022.5 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance) (2024.1)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance) (2.4.2)\n",
      "Requirement already satisfied: peewee>=3.16.2 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance) (3.17.6)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance) (4.12.3)\n",
      "Requirement already satisfied: curl_cffi>=0.7 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance) (0.11.1)\n",
      "Requirement already satisfied: protobuf>=3.19.0 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance) (4.25.3)\n",
      "Requirement already satisfied: websockets>=13.0 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance) (15.0.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/anaconda3/lib/python3.12/site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from xgboost) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (from shap) (1.5.1)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in /opt/anaconda3/lib/python3.12/site-packages (from shap) (4.66.4)\n",
      "Requirement already satisfied: packaging>20.9 in /opt/anaconda3/lib/python3.12/site-packages (from shap) (23.2)\n",
      "Requirement already satisfied: slicer==0.0.8 in /opt/anaconda3/lib/python3.12/site-packages (from shap) (0.0.8)\n",
      "Requirement already satisfied: numba>=0.54 in /opt/anaconda3/lib/python3.12/site-packages (from shap) (0.59.1)\n",
      "Requirement already satisfied: cloudpickle in /opt/anaconda3/lib/python3.12/site-packages (from shap) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/lib/python3.12/site-packages (from shap) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.5)\n",
      "Requirement already satisfied: cffi>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from curl_cffi>=0.7->yfinance) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2024.2.2 in /opt/anaconda3/lib/python3.12/site-packages (from curl_cffi>=0.7->yfinance) (2025.1.31)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /opt/anaconda3/lib/python3.12/site-packages (from numba>=0.54->shap) (0.42.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.3.0->yfinance) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.31->yfinance) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.31->yfinance) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.31->yfinance) (2.2.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->shap) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->shap) (3.5.0)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.12/site-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Install and upgrade required packages\n",
    "!pip install --upgrade yfinance ta pyspark xgboost shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Import Dependencies -----\n",
    "\n",
    "# Standard libraries\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from typing import Dict, Any, Tuple, List, Union, Sequence\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party libraries for data manipulation and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "# Scikit-learn modules for model selection, evaluation, and preprocessing\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "# Technical analysis libraries\n",
    "from ta.momentum import RSIIndicator\n",
    "from ta.trend import EMAIndicator, MACD\n",
    "\n",
    "# Suppress warnings that may clutter the output\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Fetch Stock Data & Calculate Features -----\n",
    "\n",
    "def flatten_multi_index(data: pd.DataFrame, ticker: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Flatten any two‐level columns and standardize key fields for a single ticker.\n",
    "\n",
    "    We do this so downstream code can assume simple, consistent column names\n",
    "    (no nested tuples) and rely on 'Date' being the index for time-series ops.\n",
    "\n",
    "    When data.columns is a MultiIndex of (field, ticker):\n",
    "      - Converts to \"field_ticker\".\n",
    "      - Removes the \"_{ticker}\" suffix from Date, Open, High, Low, Close.\n",
    "      - Sets Date as the index and drops the column.\n",
    "\n",
    "    Returns the modified DataFrame indexed by Date.\n",
    "    \"\"\"\n",
    "    # Check if the dataframe's columns are a MultiIndex\n",
    "    if isinstance(data.columns, pd.MultiIndex):\n",
    "        # Flatten the MultiIndex by merging levels with an underscore\n",
    "        data.columns = ['_'.join(col).strip() for col in data.columns.values]\n",
    "        \n",
    "        # Rename the flattened columns for clarity (e.g., 'Open_TICKER' -> 'Open')\n",
    "        data.rename(columns={\n",
    "            f'Date_': 'Date',\n",
    "            f'Open_{ticker}': 'Open',\n",
    "            f'High_{ticker}': 'High',\n",
    "            f'Low_{ticker}': 'Low',\n",
    "            f'Close_{ticker}': 'Close'\n",
    "        }, inplace=True)\n",
    "        \n",
    "        # Make 'Date' the index and remove its column\n",
    "        data.index = data[\"Date\"]\n",
    "        data.drop(['Date'], inplace=True, axis=1)\n",
    "        \n",
    "    return data\n",
    "\n",
    "def calculate_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate momentum and trend indicators for forecasting.\n",
    "\n",
    "    We compute short-term (1/2/3-day) and medium-term (EMA vs. SMA) signals:\n",
    "      - Momentum_1/2/3 captures recent price swings.\n",
    "      - Trend_2/3/5 highlights whether price is accelerating or decelerating.\n",
    "    We then form a composite momentum to blend volatility and direction.\n",
    "\n",
    "    Returns the original DataFrame with the new feature columns added.\n",
    "    \"\"\"\n",
    "    # Momentum: daily price differences over 1, 2, and 3 days\n",
    "    # These differences capture the change in closing prices over the specified periods.\n",
    "    df[\"Momentum_1\"] = df[\"Close\"].diff(periods=1)\n",
    "    df[\"Momentum_2\"] = df[\"Close\"].diff(periods=2)\n",
    "    df[\"Momentum_3\"] = df[\"Close\"].diff(periods=3)\n",
    "\n",
    "    # Trend: Difference between the Exponential Moving Average (EMA) and the Simple Moving Average (SMA)\n",
    "    # A positive difference indicates that EMA is above SMA, suggesting an upward trend.\n",
    "    # Using higher spans helps capture longer-term trends.\n",
    "    df['Trend_2'] = df[\"Close\"].ewm(span=2, adjust=False).mean() - df[\"Close\"].rolling(window=2).mean()\n",
    "    df['Trend_3'] = df[\"Close\"].ewm(span=3, adjust=False).mean() - df[\"Close\"].rolling(window=3).mean()\n",
    "    # Note: Trend_4 duplicates the logic of Trend_2, which might be intentional for experimentation\n",
    "    df['Trend_4'] = df[\"Close\"].ewm(span=2, adjust=False).mean() - df[\"Close\"].rolling(window=2).mean()\n",
    "    df['Trend_5'] = df[\"Close\"].ewm(span=5, adjust=False).mean() - df[\"Close\"].rolling(window=5).mean()\n",
    "\n",
    "    # Momentum is a critical feature.\n",
    "    # To aggregate momentum features from different time scales into a single composite indicator, we combine them as a weighted sum.\n",
    "    df['Momentum_Combo'] = 0.5 * df['Momentum_1'] + 0.3 * df['Momentum_2'] + 0.2 * df['Momentum_3']\n",
    "\n",
    "    return df\n",
    "\n",
    "def fetch_and_process_stock_data(ticker: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch and preprocess closing‐price data for forecasting.\n",
    "\n",
    "    Downloads daily OHLC data from Yahoo Finance, flattens any nested columns,\n",
    "    and retains only the 'Close' series. It then computes:\n",
    "      – Daily percent change to capture returns.\n",
    "      – Momentum features to reflect recent price swings.\n",
    "      – Trend features to highlight acceleration vs. deceleration.\n",
    "    Finally, it removes any rows with missing values (e.g., from rolling calculations).\n",
    "\n",
    "    Returns the cleaned, feature‐engineered DataFrame indexed by Date, ready for modeling.\n",
    "    \"\"\"\n",
    "    # 1. Download data from Yahoo Finance and reset index\n",
    "    df_yf = yf.download(ticker, start=\"2023-01-01\", end=\"2024-12-11\")\n",
    "    df_yf.reset_index(inplace=True)\n",
    "    \n",
    "    # 2. Select relevant columns\n",
    "    df = df_yf[['Date', \"Open\", \"High\", \"Low\", \"Close\"]]\n",
    "    \n",
    "    # 3. Flatten MultiIndex columns if present\n",
    "    df = flatten_multi_index(df, ticker)\n",
    "    \n",
    "    # 4. Drop unused columns\n",
    "    df.drop(['Open', 'High', 'Low'], inplace=True, axis=1)\n",
    "\n",
    "    # 5. Calculate daily percentage change\n",
    "    df[\"Change\"] = df[\"Close\"].pct_change() * 100\n",
    "\n",
    "    # 6. Compute additional features and remove missing values\n",
    "    df = calculate_features(df)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-nGIk-Xe4Zg"
   },
   "source": [
    "[\"Prev_Change_1\",\"Prev_Change_2\",\"Prev_Change_3\",\"Prev_Change_4\",\"Prev_Change_5\",\n",
    "                   \"Prev_Change_6\",\"Prev_Change_7\",\"Prev_Change_8\",\"Prev_Change_9\",\"Prev_Change_10\",\n",
    "                   \"EMA_50\", \"EMA_200\", \"RSI\",\"MACD\",\"Day_of_Week\", \"Month\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0IxTq-ayOEJt"
   },
   "outputs": [],
   "source": [
    "# ----- Forcast Stock Changes for X Weeks -----\n",
    "\n",
    "def scale_data(\n",
    "    df: pd.DataFrame\n",
    ") -> Tuple[pd.DataFrame, List[str], StandardScaler]:    \n",
    "    \"\"\"\n",
    "    Standardize momentum and trend features for modeling.\n",
    "\n",
    "    Scales the momentum and trend indicators to zero mean and unit variance so that\n",
    "    downstream models treat these features on comparable scales. \n",
    "\n",
    "    Returns the scaled DataFrame, the list of feature names that were transformed,\n",
    "    and the fitted scaler instance for any inverse transformations.\n",
    "    \"\"\"\n",
    "    # Define features and label\n",
    "    feature_columns = ['Momentum_1', \"Momentum_2\", \"Momentum_3\",\n",
    "                         'Trend_2', 'Trend_3', 'Trend_4', 'Trend_5',\n",
    "                         'Momentum_Combo']\n",
    "    \n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_array = scaler.fit_transform(df[feature_columns])\n",
    "    df[feature_columns] = scaled_array\n",
    "\n",
    "    return df, feature_columns, scaler\n",
    "\n",
    "def update_with_predicted_close(\n",
    "    df: pd.DataFrame,\n",
    "    pred: Union[np.ndarray, pd.Series, Sequence[float]]\n",
    ") -> pd.DataFrame:    \n",
    "    \"\"\"\n",
    "    Append the next trading day's predicted close and refresh features.\n",
    "\n",
    "    This extends the series one step at a time using only the forecasted percent change,\n",
    "    ensuring that all feature computations rely solely on available or predicted values.\n",
    "    By recalculating features after each prediction and never peeking at actual future\n",
    "    prices, we prevent data leakage into the model inputs.\n",
    "\n",
    "    Steps:\n",
    "      - Compute the new Close using the last known price and pred[0].\n",
    "      - Advance to the next trading day (skipping the weekend if needed).\n",
    "      - Insert a new row with the predicted Close.\n",
    "      - Recalculate daily returns and momentum/trend features over the extended series.\n",
    "\n",
    "    Returns the DataFrame with the added prediction and updated feature columns.\n",
    "    \"\"\"\n",
    "    # 1. Compute the new predicted 'Close' price based on the last known close\n",
    "    new_close = df['Close'].iloc[-1] * (1 + (pred[0] / 100))\n",
    "\n",
    "    # 2. Determine the next trading day: if it's Friday, jump to Monday; else next day\n",
    "    if df.index[-1].dayofweek == 4:\n",
    "        next_date = df.index[-1] + pd.Timedelta(days=3)\n",
    "    else:\n",
    "        next_date = df.index[-1] + pd.Timedelta(days=1)\n",
    "\n",
    "    # 3. Duplicate the last row, replace 'Close' with the new value, and assign to next_date\n",
    "    new_row = df.iloc[-1].copy()\n",
    "    new_row['Close'] = new_close\n",
    "    df.loc[next_date] = new_row\n",
    "\n",
    "    # 4. Recompute daily percentage changes and recalculate features\n",
    "    df[\"Change\"] = df[\"Close\"].pct_change() * 100\n",
    "    df = calculate_features(df)\n",
    "\n",
    "    return df\n",
    "    \n",
    "def forecast_stock_changes(\n",
    "    df: pd.DataFrame,\n",
    "    scaler: StandardScaler,\n",
    "    ts: int,\n",
    "    model: Any,\n",
    "    features: List[str]\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Iteratively simulate live forecasts of percentage changes over `ts` steps.\n",
    "\n",
    "    This rolling‐window approach mimics real‐time forecasting and prevents data leakage:\n",
    "      - Predict the next period’s change using only the most recent scaled features.\n",
    "      - Inverse‐transform features back to their original scale before inserting the prediction.\n",
    "      - Append the predicted 'Close' and recompute features via `update_with_predicted_close`.\n",
    "      - Reapply scaling to the extended dataset before the next prediction.\n",
    "\n",
    "    Returns the 1D array of predicted 'Change' values for the forecast horizon.\n",
    "    \"\"\"\n",
    "    df_forecast = df.copy()\n",
    "    for i in range(ts):\n",
    "        # Use the last row of features to generate a prediction.\n",
    "        X = df_forecast[features].iloc[-1:]\n",
    "        pred = model.predict(X)\n",
    "\n",
    "        # Convert features back to original scale before updating.\n",
    "        df_forecast[features] = scaler.inverse_transform(df_forecast[features])\n",
    "        \n",
    "        # Insert the new predicted 'Close' and update features.\n",
    "        df_forecast = update_with_predicted_close(df_forecast, pred)\n",
    "        \n",
    "        # Re-scale the updated features for the next prediction.\n",
    "        df_forecast[features] = scaler.transform(df_forecast[features])\n",
    "\n",
    "    # Return the predicted 'Change' values for the forecasted steps.\n",
    "    return df_forecast['Change'].iloc[-ts:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "xXE__O40V2GO"
   },
   "outputs": [],
   "source": [
    "# ----- Train N XGBoost Models for N Stock Tickers -----\n",
    "\n",
    "def train_models(\n",
    "    tickers: List[str],\n",
    "    weeks: int\n",
    ") -> Tuple[Dict[str, BaseEstimator], Dict[str, Dict[str, float]]]: \n",
    "    \"\"\"\n",
    "    Train and evaluate XGBoost forecasting models for multiple tickers.\n",
    "\n",
    "    For each ticker, this function:\n",
    "      - Fetches and preprocesses historical price data.\n",
    "      - Scales features and fits an XGBoost model.\n",
    "      - Performs rolling forecasts over `weeks` successive windows.\n",
    "      - Records actual vs. predicted percent changes.\n",
    "\n",
    "    It returns two mappings:\n",
    "      1. model_metadata: ticker → {'model': estimator, 'scaler': scaler, 'features': [...]}\n",
    "      2. evaluation:     ticker → week_index → {'Dates', 'actual_change', 'predicted_change'}\n",
    "    \"\"\"\n",
    "    # Initialize dictionaries to store metadata (model, scaler, features) and evaluation results.\n",
    "    model_metadata = {}\n",
    "    evaluation = {}\n",
    "\n",
    "    # Create an evaluation sub-dictionary for each ticker if not present.\n",
    "    for ticker in tickers:\n",
    "        if ticker not in evaluation:\n",
    "            evaluation[ticker] = {}\n",
    "\n",
    "    # Process each ticker individually\n",
    "    for ticker in tickers:\n",
    "        # Load and prepare the data\n",
    "        df = fetch_and_process_stock_data(ticker)\n",
    "        df, feature_columns, scaler = scale_data(df)\n",
    "\n",
    "        # Train the model on this ticker's data\n",
    "        xgboost_model = train_and_evaluate(df, feature_columns, weeks)\n",
    "        \n",
    "        # Store the model, scaler, and feature columns\n",
    "        model_metadata[ticker] = {\n",
    "            \"model\": xgboost_model,\n",
    "            \"scaler\": scaler,\n",
    "            \"features\": feature_columns\n",
    "        }\n",
    "\n",
    "        # Generate and record evaluation metrics for each week in the specified range\n",
    "        for week in range(weeks):\n",
    "            # Calculate how many rows to go back (x) to retrieve actual data and make predictions\n",
    "            x = (weeks + 2 - week) * 5\n",
    "            \n",
    "            # Subset df to exclude the last x rows, then keep the relevant portion for predictions\n",
    "            df_forecast = df.iloc[:-x+5]\n",
    "            \n",
    "            # Get actual changes and forecast them with the model\n",
    "            actual_diffs = df.iloc[-x+5:-x+10]['Change'].values\n",
    "            pred = forecast_stock_changes(df_forecast, scaler, 5, xgboost_model, feature_columns)\n",
    "\n",
    "            # Convert arrays to lists for easier serialization or storage\n",
    "            actual_diffs = actual_diffs.tolist()\n",
    "            pred = pred.tolist()\n",
    "\n",
    "            # Retrieve corresponding dates for the actual changes\n",
    "            actual_dates = df.reset_index()\n",
    "            actual_dates = actual_dates['Date'].iloc[-x+5:-x+10]\n",
    "\n",
    "            # Record results (dates, actual changes, predicted changes) for this week\n",
    "            evaluation[ticker][week + 1] = {\n",
    "                'Dates': [str(date)[:10] for date in actual_dates],\n",
    "                \"actual_change\": actual_diffs,\n",
    "                \"predicted_change\": pred\n",
    "            }\n",
    "\n",
    "    return model_metadata, evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KGOeVAGtXWTv"
   },
   "outputs": [],
   "source": [
    "# ----- Train Individual XGBoost Model for Each Stock Ticker Over X Weeks -----\n",
    "\n",
    "def train_and_evaluate(\n",
    "    df: pd.DataFrame,\n",
    "    features: List[str],\n",
    "    weeks: int\n",
    ") -> BaseEstimator:    \n",
    "    \"\"\"\n",
    "    Train an XGBoost regressor on historical features, excluding a hold-out block.\n",
    "\n",
    "    To prevent lookahead bias, the final (weeks + 1) * 5 rows are dropped and never seen\n",
    "    during training. The model is then fitted on all prior data.\n",
    "\n",
    "    Steps:\n",
    "      - Compute hold-out size = (weeks + 1) * 5 rows.\n",
    "      - Drop those rows from the DataFrame.\n",
    "      - Train an XGBRegressor on the remaining data.\n",
    "\n",
    "    Returns the trained XGBoost regressor.\n",
    "    \"\"\"\n",
    "    # Calculate how many rows to exclude from the end for hold-out\n",
    "    past = (weeks + 1) * 5\n",
    "    \n",
    "    # Truncate the DataFrame to exclude future rows\n",
    "    df = df.iloc[:-past]\n",
    "\n",
    "    # Separate features (X) and target (y)\n",
    "    X = df[features]\n",
    "    y = df[\"Change\"]\n",
    "\n",
    "    # Initialize XGBoost regressor with chosen hyperparameters\n",
    "    xgboost = xgb.XGBRegressor(\n",
    "        max_depth=4,\n",
    "        learning_rate=0.01,\n",
    "        n_estimators=1000,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=1.0,\n",
    "    )\n",
    "\n",
    "    # Train the model on the available data\n",
    "    xgboost.fit(X, y)\n",
    "\n",
    "    return xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "VkyN5BIGeVKT"
   },
   "outputs": [],
   "source": [
    "# ----- Plot Feature Importance for Each Model -----\n",
    "\n",
    "def plot_feature_importance(\n",
    "    model: Any,\n",
    "    feature_names: List[str],\n",
    "    figsize: Tuple[int, int] = (10, 6)\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Plot feature importances and return them as a sorted DataFrame.\n",
    "\n",
    "    Displays a horizontal bar chart (largest importance on top) and returns\n",
    "    a DataFrame with columns ['Feature', 'Importance'] sorted descending.\n",
    "    \"\"\"\n",
    "    feature_importances = model.feature_importances_\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': feature_importances\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.barh(importance_df['Feature'], importance_df['Importance'], align='center')\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.ylabel('Features')\n",
    "    plt.title('Feature Importance')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "\n",
    "    return importance_df\n",
    "\n",
    "def plot_importances_for_tickers(\n",
    "    ticker_sample: List[str],\n",
    "    metadata: Dict[str, Any]\n",
    ") -> Figure:    \n",
    "    \"\"\"\n",
    "    Plot feature importances for each ticker in the sample.\n",
    "\n",
    "    For each symbol in `ticker_sample`, this invokes the base\n",
    "    `plot_importances_for_tickers(model, features)` function to display\n",
    "    its feature‐importance chart.\n",
    "    \"\"\"\n",
    "    for stock in ticker_sample:\n",
    "        m = metadata[stock]['model']\n",
    "        f = metadata[stock]['features']\n",
    "        plot_feature_importance(m, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hEINBKmfXEqv"
   },
   "source": [
    ":Beste Parameter: {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 1000, 'reg_alpha': 0.1, 'reg_lambda': 1.0, 'subsample': 0.8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "dnihbv4T7f1G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/benjamingreif/Documents/DocumentsLocal/dataScience/GitHub_Profil/AbschlussprojektNew\n"
     ]
    }
   ],
   "source": [
    "# Print the current working directory to get filepath\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "nR13b-QBq7SO"
   },
   "outputs": [],
   "source": [
    "# ----- Plot Performance Metrics of all Models -----\n",
    "\n",
    "def calculate_metrics(\n",
    "    json_file: Union[str, Path, Dict[str, Any]],\n",
    "    ticker: str\n",
    ") -> Tuple[List[Tuple[int, float, float]], float, float]:\n",
    "    \"\"\"\n",
    "    Compute weekly and overall accuracy and error for a ticker’s forecasts.\n",
    "\n",
    "    Reads prediction data and, for each week, calculates:\n",
    "      – ADA (directional accuracy)  \n",
    "      – MAE (mean absolute error)  \n",
    "\n",
    "    Returns a tuple of (weekly_metrics, total_ada, overall_mae).\n",
    "    \"\"\"\n",
    "    with open(json_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    stock = ticker\n",
    "    total_correct_directions = 0\n",
    "    total_predictions = 0\n",
    "    total_absolute_error = 0\n",
    "    weekly_metrics = []\n",
    "\n",
    "    for week, values in data[stock].items():\n",
    "        actual_changes = np.array(values[\"actual_change\"])\n",
    "        predicted_changes = np.array(values[\"predicted_change\"])\n",
    "\n",
    "        # Calculate Directional Accuracy (Hit Rate)\n",
    "        correct_directions = np.sum(\n",
    "            (actual_changes > 0) == (predicted_changes > 0)\n",
    "        )\n",
    "        ada = correct_directions / len(actual_changes)\n",
    "        total_correct_directions += correct_directions\n",
    "        total_predictions += len(actual_changes)\n",
    "\n",
    "        # Calculate MAE\n",
    "        mae = np.mean(np.abs(actual_changes - predicted_changes))\n",
    "        total_absolute_error += np.sum(np.abs(actual_changes - predicted_changes))\n",
    "        weekly_metrics.append((week, ada, mae))\n",
    "\n",
    "    total_ada = total_correct_directions / total_predictions\n",
    "    overall_mae = total_absolute_error / total_predictions\n",
    "\n",
    "    return weekly_metrics, total_ada, overall_mae\n",
    "\n",
    "def plot_forecast_performance(\n",
    "    json_file: Union[str, Path],\n",
    "    tickers: List[str]\n",
    ") -> None:    \n",
    "    \"\"\"\n",
    "    Display weekly and overall ADA and MAE for each ticker from a JSON file.\n",
    "\n",
    "    Reads `json_file` (a path to a JSON file mapping tickers → weekly predictions)\n",
    "    and, for each symbol in `tickers`, prints:\n",
    "      - Directional accuracy (ADA) and MAE for each week\n",
    "      - Overall ADA and MAE across all weeks\n",
    "\n",
    "    Finally, prints the average ADA/MAE aggregated across all tickers.\n",
    "    \"\"\"\n",
    "    total_overall_ada = []\n",
    "    total_overall_mae = []\n",
    "\n",
    "    for stock in tickers:\n",
    "\n",
    "        weekly_results, overall_ada, overall_mae = calculate_metrics(json_file, stock)\n",
    "        total_overall_ada.append(overall_ada)\n",
    "        total_overall_mae.append(overall_mae)\n",
    "\n",
    "        print(f\"\\n{stock} Weekly Metrics (Week, ADA/MAE):\")\n",
    "        for week, ada, mae in weekly_results:\n",
    "            print(f\"Week {week}: ADA = {ada:.2f}, MAE = {mae:.2f}%\")\n",
    "\n",
    "        print(f\"\\n{stock} Overall Average Directional Accuracy: {overall_ada:.2f}\")\n",
    "        print(f\"{stock} Overall MAE: {overall_mae:.2f}\")\n",
    "\n",
    "    print(f\"\\nTotal Average Directional Accuracy across all {len(tickers)} Stocks: {np.mean(total_overall_ada):.2f}\")\n",
    "    print(f\"Total Average MAE across all {len(tickers)} Stocks: {np.mean(total_overall_mae):.2f}%\")\n",
    "\n",
    "def compute_average_directional_accuracy(\n",
    "    file_path: Union[str, Path]\n",
    ") -> Dict[int, float]:   \n",
    "    \"\"\"\n",
    "    Compute average directional accuracy for the first five forecast days.\n",
    "\n",
    "    Reads a JSON of weekly actual vs. predicted changes and returns a mapping\n",
    "    from day index (0–4) to the fraction of times the predicted and actual \n",
    "    change directions matched across all stocks and weeks.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    ada_per_day = {day: [] for day in range(5)}\n",
    "\n",
    "    for stock, weeks in data.items():\n",
    "        for week, week_data in weeks.items():\n",
    "            actual_changes = week_data['actual_change']\n",
    "            predicted_changes = week_data['predicted_change']\n",
    "\n",
    "            for day in range(len(actual_changes)):\n",
    "                if day < 5:  # Ensure we stay within the first 5 prediction days\n",
    "                    actual = actual_changes[day]\n",
    "                    predicted = predicted_changes[day]\n",
    "\n",
    "                    # Check if the directions match and record the result\n",
    "                    ada_per_day[day].append((actual >= 0 and predicted >= 0) or (actual < 0 and predicted < 0))\n",
    "\n",
    "    # Calculate the average directional accuracy for each day\n",
    "    average_ada = {}\n",
    "    for day, results in ada_per_day.items():\n",
    "        average_ada[day] = sum(results) / len(results) if results else 0\n",
    "\n",
    "    return average_ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "AFjtI7Mrom6-"
   },
   "outputs": [],
   "source": [
    "# ----- Define Balanced Stock Sample by Industry Segments -----\n",
    "\n",
    "ticker_sample = [\n",
    "    # E-commerce/Consumer Staples\n",
    "    \"KO\",         # The Coca-Cola Company (Beverages, Consumer Staples)\n",
    "    \"AMZN\",       # Amazon.com Inc. (E-Commerce, Cloud Computing)\n",
    "\n",
    "    # Technology\n",
    "    \"IBM\",        # International Business Machines Corporation (Cloud Computing, AI, Hardware)\n",
    "    \"SIE.DE\",     # Siemens AG (Industrial Manufacturing, Automation)\n",
    "\n",
    "    # Automotive/Electric Vehicles\n",
    "    \"TSLA\",       # Tesla Inc. (Electric Vehicles, Renewable Energy)\n",
    "    \"VOW3.DE\",    # Volkswagen AG (Automotive Manufacturing)\n",
    "\n",
    "    # Financial Services\n",
    "    \"V\",          # Visa Inc. (Payment Processing, Financial Services)\n",
    "    \"MUV2.DE\",    # Munich Re (Münchener Rückversicherungs-Gesellschaft) (Reinsurance)\n",
    "\n",
    "    # Chemicals\n",
    "    \"BAS.DE\",     # BASF SE (Chemical Manufacturing)\n",
    "    \"BAYN.DE\"     # Bayer AG (Pharmaceuticals, Chemicals)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Setup Stock Forecasting Pipeline -----\n",
    "\n",
    "def run_forecasting(\n",
    "    weeks: int,\n",
    "    tickers: List[str],\n",
    ") -> Tuple[Dict[str, BaseEstimator], Dict[str, Dict[str, float]]]:\n",
    "    \"\"\"\n",
    "    Train models and gather raw forecast results for each ticker.\n",
    "\n",
    "    Calls `train_models` and returns two mappings:\n",
    "      - metadata: ticker → {'model': estimator, 'scaler': scaler, 'features': [...]}\n",
    "      - evaluation: ticker → week → {\n",
    "            'Dates': [...], \n",
    "            'actual_change': [...], \n",
    "            'predicted_change': [...]\n",
    "        }\n",
    "    \"\"\"\n",
    "    metadata, evaluation = train_models(tickers, weeks)\n",
    "    return metadata, evaluation\n",
    "\n",
    "def plot_forecasting_results(\n",
    "    weeks: int,\n",
    "    tickers: List[str],\n",
    "    metadata: Dict[str, BaseEstimator],\n",
    "    evaluation: Dict[str, Dict[str, float]]\n",
    ") -> None:    \n",
    "    \"\"\"\n",
    "    Save evaluation results, display forecasts, and print average directional accuracy.\n",
    "\n",
    "    - Writes `evaluation` to \"stock_sample_predictions_{weeks}_weeks_new.json\".\n",
    "    - Calls `plot_forecast_performance` to show actual vs. predicted changes.\n",
    "    - Computes and prints the average directional accuracy for each forecast day.\n",
    "    \"\"\"\n",
    "    file_path = f\"stock_sample_predictions_{weeks}_weeks.json\"\n",
    "    print(f\"Forecasting {len(tickers)} stocks for {weeks} weeks...\")\n",
    "\n",
    "    # If interested in metadata\n",
    "    #print(\"Model metadata:\", metadata)\n",
    "    #print(\"Evaluation results:\", evaluation)\n",
    "\n",
    "    # If interested plot feature importances\n",
    "    #plot_importances_for_tickers(ticker_sample, metadata)\n",
    "\n",
    "    # Write the evaluation data to the JSON file first.\n",
    "    with open(file_path, \"w\") as json_file:\n",
    "        json.dump(evaluation, json_file, indent=4)\n",
    "\n",
    "    # If interested plot feature importance\n",
    "    plot_forecast_performance(file_path, tickers)\n",
    "\n",
    "    # print avverage directional accuracy rate\n",
    "    ada_results = compute_average_directional_accuracy(file_path)\n",
    "    print(\"\\nAverage Directional Accuracy per Day:\")\n",
    "    for day, ada in ada_results.items():\n",
    "        print(f\"Day {day+1}: {ada:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YF.download() has changed argument auto_adjust default to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# ----- Run Stock Forecasting Pipeline -----\n",
    "\n",
    "weeks = 6\n",
    "metadata, evaluation = run_forecasting(weeks, ticker_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecasting 10 stocks for 6 weeks...\n",
      "\n",
      "KO Weekly Metrics (Week, ADA/MAE):\n",
      "Week 1: ADA = 1.00, MAE = 2.03%\n",
      "Week 2: ADA = 0.60, MAE = 0.69%\n",
      "Week 3: ADA = 0.40, MAE = 0.91%\n",
      "Week 4: ADA = 0.80, MAE = 0.70%\n",
      "Week 5: ADA = 1.00, MAE = 0.62%\n",
      "Week 6: ADA = 0.40, MAE = 0.92%\n",
      "\n",
      "KO Overall Average Directional Accuracy: 0.70\n",
      "KO Overall MAE: 0.98\n",
      "\n",
      "AMZN Weekly Metrics (Week, ADA/MAE):\n",
      "Week 1: ADA = 0.80, MAE = 0.94%\n",
      "Week 2: ADA = 0.60, MAE = 2.53%\n",
      "Week 3: ADA = 0.40, MAE = 3.07%\n",
      "Week 4: ADA = 0.60, MAE = 2.23%\n",
      "Week 5: ADA = 0.60, MAE = 1.80%\n",
      "Week 6: ADA = 0.80, MAE = 5.02%\n",
      "\n",
      "AMZN Overall Average Directional Accuracy: 0.63\n",
      "AMZN Overall MAE: 2.60\n",
      "\n",
      "IBM Weekly Metrics (Week, ADA/MAE):\n",
      "Week 1: ADA = 0.60, MAE = 1.66%\n",
      "Week 2: ADA = 0.60, MAE = 1.58%\n",
      "Week 3: ADA = 0.20, MAE = 3.03%\n",
      "Week 4: ADA = 0.60, MAE = 0.96%\n",
      "Week 5: ADA = 1.00, MAE = 1.72%\n",
      "Week 6: ADA = 0.60, MAE = 1.07%\n",
      "\n",
      "IBM Overall Average Directional Accuracy: 0.60\n",
      "IBM Overall MAE: 1.67\n",
      "\n",
      "SIE.DE Weekly Metrics (Week, ADA/MAE):\n",
      "Week 1: ADA = 0.60, MAE = 1.29%\n",
      "Week 2: ADA = 0.60, MAE = 1.18%\n",
      "Week 3: ADA = 0.60, MAE = 2.90%\n",
      "Week 4: ADA = 0.60, MAE = 4.85%\n",
      "Week 5: ADA = 0.60, MAE = 4.34%\n",
      "Week 6: ADA = 0.40, MAE = 1.73%\n",
      "\n",
      "SIE.DE Overall Average Directional Accuracy: 0.57\n",
      "SIE.DE Overall MAE: 2.72\n",
      "\n",
      "TSLA Weekly Metrics (Week, ADA/MAE):\n",
      "Week 1: ADA = 0.60, MAE = 6.04%\n",
      "Week 2: ADA = 1.00, MAE = 2.42%\n",
      "Week 3: ADA = 0.00, MAE = 11.62%\n",
      "Week 4: ADA = 0.60, MAE = 13.61%\n",
      "Week 5: ADA = 0.40, MAE = 12.68%\n",
      "Week 6: ADA = 0.60, MAE = 7.82%\n",
      "\n",
      "TSLA Overall Average Directional Accuracy: 0.53\n",
      "TSLA Overall MAE: 9.03\n",
      "\n",
      "VOW3.DE Weekly Metrics (Week, ADA/MAE):\n",
      "Week 1: ADA = 0.20, MAE = 1.53%\n",
      "Week 2: ADA = 0.80, MAE = 2.01%\n",
      "Week 3: ADA = 0.60, MAE = 1.80%\n",
      "Week 4: ADA = 0.60, MAE = 1.29%\n",
      "Week 5: ADA = 0.80, MAE = 0.73%\n",
      "Week 6: ADA = 0.80, MAE = 0.74%\n",
      "\n",
      "VOW3.DE Overall Average Directional Accuracy: 0.63\n",
      "VOW3.DE Overall MAE: 1.35\n",
      "\n",
      "V Weekly Metrics (Week, ADA/MAE):\n",
      "Week 1: ADA = 0.80, MAE = 1.37%\n",
      "Week 2: ADA = 0.60, MAE = 1.63%\n",
      "Week 3: ADA = 0.80, MAE = 1.14%\n",
      "Week 4: ADA = 0.40, MAE = 1.58%\n",
      "Week 5: ADA = 0.60, MAE = 1.54%\n",
      "Week 6: ADA = 0.60, MAE = 1.88%\n",
      "\n",
      "V Overall Average Directional Accuracy: 0.63\n",
      "V Overall MAE: 1.52\n",
      "\n",
      "MUV2.DE Weekly Metrics (Week, ADA/MAE):\n",
      "Week 1: ADA = 0.60, MAE = 2.16%\n",
      "Week 2: ADA = 0.60, MAE = 1.21%\n",
      "Week 3: ADA = 0.20, MAE = 2.29%\n",
      "Week 4: ADA = 0.60, MAE = 3.11%\n",
      "Week 5: ADA = 1.00, MAE = 0.72%\n",
      "Week 6: ADA = 0.40, MAE = 1.09%\n",
      "\n",
      "MUV2.DE Overall Average Directional Accuracy: 0.57\n",
      "MUV2.DE Overall MAE: 1.76\n",
      "\n",
      "BAS.DE Weekly Metrics (Week, ADA/MAE):\n",
      "Week 1: ADA = 0.80, MAE = 0.54%\n",
      "Week 2: ADA = 0.60, MAE = 1.25%\n",
      "Week 3: ADA = 0.60, MAE = 3.59%\n",
      "Week 4: ADA = 0.60, MAE = 6.24%\n",
      "Week 5: ADA = 0.60, MAE = 0.97%\n",
      "Week 6: ADA = 0.40, MAE = 2.20%\n",
      "\n",
      "BAS.DE Overall Average Directional Accuracy: 0.60\n",
      "BAS.DE Overall MAE: 2.46\n",
      "\n",
      "BAYN.DE Weekly Metrics (Week, ADA/MAE):\n",
      "Week 1: ADA = 0.40, MAE = 1.03%\n",
      "Week 2: ADA = 1.00, MAE = 0.39%\n",
      "Week 3: ADA = 0.60, MAE = 4.23%\n",
      "Week 4: ADA = 0.80, MAE = 2.54%\n",
      "Week 5: ADA = 0.80, MAE = 2.02%\n",
      "Week 6: ADA = 0.60, MAE = 1.41%\n",
      "\n",
      "BAYN.DE Overall Average Directional Accuracy: 0.70\n",
      "BAYN.DE Overall MAE: 1.94\n",
      "\n",
      "Total Average Directional Accuracy across all 10 Stocks: 0.62\n",
      "Total Average MAE across all 10 Stocks: 2.60%\n",
      "\n",
      "Average Directional Accuracy per Day:\n",
      "Day 1: 0.68\n",
      "Day 2: 0.48\n",
      "Day 3: 0.45\n",
      "Day 4: 0.65\n",
      "Day 5: 0.65\n"
     ]
    }
   ],
   "source": [
    "# ----- Plot Stock Forecasting Results -----\n",
    "plot_forecasting_results(weeks, ticker_sample, metadata, evaluation)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
